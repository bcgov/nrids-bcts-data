services:
  postgres:
    build: 
      context: .
      dockerfile: Dockerfile.postgres
    container_name: postgres_ods
    environment:
      - POSTGRES_HOST_AUTH_METHOD=trust  # Temporary for development purposes
      - PGHOST=0.0.0.0  # Bind to all addresses
    env_file:
      - ./.env
    ports:
      - "5432:5432"
    volumes:
      - ./init-scripts/init_postgres.sh:/docker-entrypoint-initdb.d/init_postgres.sh
      - ./init-scripts/init_postgres_schema.sql:/docker-entrypoint-initdb.d/init_postgres_schema.sql
      - postgres_data:/var/lib/postgresql/data  # Volume for data persistence
    profiles:
      - postgres_ods
      - airflow
    networks:
      - ods_network  # Use the shared network

  oracle:
    build:
      context: .
      dockerfile: Dockerfile.oracle
    container_name: oracle_lrm
    env_file:
      - ./.env
    environment:
      ORACLE_PASSWORD: ${ORACLE_PASSWORD}
    ports:
      - "1521:1521"
    volumes:
      - ./init-scripts:/init-scripts 
    profiles:
      - oracle_lrm
    networks:
      - ods_network  # Use the shared network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    env_file:
      - ./.env
    ports:
      - "8080:8080"  # Airflow UI port
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}  # Connect to Postgres
      - AIRFLOW__DATABASE_NAME=postgres_ods  # Set the Airflow database name
      - AIRFLOW_USERNAME=${AIRFLOW_USERNAME}
      - AIRFLOW_PASSWORD=${AIRFLOW_PASSWORD}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./dags:/opt/airflow/dags  # Mount for your DAGs
      - ./logs:/opt/airflow/logs  # Mount for logs
      - ./plugins:/opt/airflow/plugins  # Mount for plugins
      - ~/.kube/config:/home/airflow/.kube/config
    depends_on:
      - postgres
    entrypoint: /usr/local/bin/entrypoint_webserver.sh  # Use the webserver entry point
    networks:
      - ods_network  # Use the shared network

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    env_file:
      - ./.env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}  # Connect to Postgres
      - AIRFLOW__DATABASE_NAME=postgres_ods  # Set the Airflow database name
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./dags:/opt/airflow/dags  # Mount for your DAGs
      - ./logs:/opt/airflow/logs  # Mount for logs
      - ./plugins:/opt/airflow/plugins  # Mount for plugins
      - ~/.kube/config:/home/airflow/.kube/config
    depends_on:
      - postgres
    entrypoint: /usr/local/bin/entrypoint_scheduler.sh  # Use the scheduler entry point
    networks:
      - ods_network  # Use the shared network


volumes:
  postgres_data:  # Named volume for Postgres

networks:
  ods_network:  # Define the shared network
    driver: bridge



